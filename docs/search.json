[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jack T. Rametta",
    "section": "",
    "text": "Hello there!\nI’m a PhD candidate in Political Science at the University of California, Davis with subfields in methods and American politics.\nI employ causally oriented machine learning methods to study American political institutions and public policy. My dissertation research focuses on Congressional oversight and policymaking capacity, as well as inter-branch politics.\nI’m also interested in the development and application of new methods, as well as accompanying software tools, for difficult social science problems. I have several projects in-progress in this vein with co-authors.\nBefore graduate school, I was an economic policy research analyst at the Bipartisan Policy Center. At BPC, I contributed to research on the federal budget, debt limit, Social Security and Medicare financing, and defense personnel reform."
  },
  {
    "objectID": "posts/2023-08-12-conformal/index.html",
    "href": "posts/2023-08-12-conformal/index.html",
    "title": "Conformalized Local Linear Forests",
    "section": "",
    "text": "This post briefly demonstrates how to generate conformal uncertainty bands/intervals in R employing the cfcausal package wrapped around the local linear forest estimator from the grf package. I don’t spend much time here explaining how conformal intervals are constructed, for that see (Lei et al. 2018) and (Samii 2019).\nFor the purpose of this demo, we’ll use a modified version of the data generating process (DGP) from (Friedman 1991), namely:\n\\[y=10 \\sin (\\pi x 1 x 2)+20(x 3-0.5)^2+10 x 4+5 x 5+e\\]where \\(e \\sim N(0,1)\\). In addition to the five covariates that are related to the outcome, there are also 25 noise covariates unrelated to the outcome (all \\(\\sin N(0,1)\\)). This particular DGP as proved challenging for the Gaussian confidence intervals constructed using the standard local linear forest method (see the Monte Carlo simulations presented in (Friedberg et al. 2020), equation 7).\nDGP simulated below:\n\n# Packages (apologies, this is dependency heavy script!)  \npacman::p_load(\"mlbench\",\"grf\",\"cfcausal\",\"caret\",\n               \"reshape2\",\"glmnet\",\"tidyverse\", \"ggthemes\",\n               \"ggdist\",\"ggpubr\")\n\n\n# Set seed for reproducibility \nset.seed(1995)\n# Simulate the DGP\np <- 20 #20 additional predictor vars \nn <- 500 #sample size  \njunk <- matrix(rnorm(n * p), n, p) #junk predictors \ndata <- mlbench::mlbench.friedman1(n) #friedman MARs DGP...\ndata <- data.frame(y = data$y, x = data$x,junk) #...adding more junk\n#\n# Split train/test set\ntrainIndex <- caret::createDataPartition(data$y, p=0.7, list = FALSE) #70/30 split \ntrain <- data[trainIndex,]\ntest <- data[-trainIndex,]\n#\n# Convenience objects \nY <- train$y\nY.test <- test$y\nX <- train[,-1]\nX.test <- test[,-1]\n# \n\nNow we can move on to training the local linear forest, we’ll enable the local linear split feature and use cross-validated lasso to select the correction features.\n\n# local linear regression forest with LL splits enabled \nc.forest.ll <- grf::ll_regression_forest(X = as.matrix(X), Y = Y, \n                                    #tune.parameters = \"all\", #can't have this with ll splits, IRL you'd want to do a custom tuning loop \n                                    enable.ll.split = TRUE, ll.split.weight.penalty = TRUE,\n                                    num.trees = 4000, #upping from default for stable variance estimates \n                                    seed = 1995) \n# \n# Select covariates for local linear correction \nlasso.mod  <- glmnet::cv.glmnet(as.matrix(X), Y, alpha = 1,  nfolds = 20) #cross-validated lasso \nlasso.coef <- predict(lasso.mod, type = \"nonzero\")\nselected   <- lasso.coef[,1] \n#\n# out-of-sample preds, could also look at oob preds (leave out the testing set and use X instead)\npreds.ll <- predict(c.forest.ll,X.test,estimate.variance = TRUE,linear.correction.variables = selected)\n# data frame for plots later, adding built-in grf uncertainty intervals \nplot.df <- data.frame(ll.preds   = preds.ll$predictions,\n                      ll.upper   = preds.ll$predictions + 1.96*sqrt(preds.ll$variance.estimates), #grf 95% confidence intervals\n                      ll.lower   = preds.ll$predictions - 1.96*sqrt(preds.ll$variance.estimates),\n                      Y = Y.test)\n# column to indicate whether the uncertainty band contains the true value (for plotting)\nplot.df$ll.cover   <- as.factor(ifelse(Y.test >= plot.df$ll.lower & Y.test <= plot.df$ll.upper,1,0))\n\nIn order to generate the conformal intervals, we first need to setup a function that will estimate the local linear forest model.\n\n# Setup the llf function to plugin to cfcausal, same settings and seed as above\nllRF <- function(Y, X, Xtest, ...){\n  fit <- grf::ll_regression_forest(X, Y,  enable.ll.split = TRUE,\n                                   ll.split.weight.penalty = TRUE,num.trees = 4000,seed = 1995,...)\n  \n  # Same selection procedure \n  lasso.mod <- glmnet::cv.glmnet(as.matrix(X), Y, alpha = 1,  nfolds = 20) \n  lasso.coef <- predict(lasso.mod, type = \"nonzero\")\n  selected <- lasso.coef[,1] \n  #\n  # out-of sample preds\n  res <- predict(c.forest.ll,Xtest,estimate.variance = FALSE,linear.correction.variables = selected) #turn off grf variance estimates \n  # \n  res <- as.numeric(res$predictions)\n  return(res)\n}\n\nWe can then feed that function into the cfcausal::conformal function to generate unweighted standard conformal intervals.\n\n# Setup the conformal prediction function plugging in our llf estimator \nc.test <- cfcausal::conformal(X = X,Y = Y, type = \"mean\", side = \"two\", \n                              wtfun = NULL, #unweighted \n                              outfun = llRF, #our custom output function\n                              useCV = FALSE) # Note: we're using split conformal here, you could alternatively use CV+ by setting useCV = FALSE. \nll.preds.conformal <- predict(c.test,X.test,alpha = .025) #generate the uncertainty bands, here we're .025\n# Save out the results \nplot.df$ll.upper.c <- ll.preds.conformal$upper\nplot.df$ll.lower.c <- ll.preds.conformal$lower\n# Column for whether truth is covered or not in a given instance, for plotting \nplot.df$ll.cover.c <- as.factor(ifelse(Y.test >= plot.df$ll.lower.c & Y.test <= plot.df$ll.upper.c,1,0))\n\nNow we can plot a comparison between the local linear forest model predictions wrapped in the standard Gaussian and conformal intervals (both aiming for 95% coverage).\n\n# llf plot with grf uncertainty bands \nll.plot <- plot.df %>% \n           ggplot(aes(y = ll.preds, x = Y.test, ymin = ll.lower, ymax = ll.upper,color = ll.cover)) + \n           geom_pointinterval(alpha = .5,shape = 1) + \n           scale_x_continuous(limits = c(0,30), expand = c(0, 0)) +\n           scale_y_continuous(limits = c(0,30), expand = c(0, 0)) +\n           geom_abline(linewidth = .75, intercept = 0,slope = 1) + \n           ylab(\"LLF Predictions (Y), 95% Confidence Intervals\") + \n           xlab(\"Y (Real, Test Set)\") + \n           ggtitle(\"Local Linear Forests\") +\n           ggthemes::theme_few()+ \n           scale_color_manual(values = c(\"firebrick1\",\"dodgerblue\")) + \n           guides(color=guide_legend(title=\"Cover Truth? (Blue = Yes)\"))\n# llf plot with conformal bands \nll.plot.c <- plot.df %>% \n             ggplot(aes(y = ll.preds, x = Y.test, ymin = ll.lower.c, ymax = ll.upper.c,color = ll.cover.c)) + \n             geom_pointinterval(alpha = .5,shape = 1) + \n             scale_x_continuous(limits = c(0,30), expand = c(0, 0)) +\n             scale_y_continuous(limits = c(0,30), expand = c(0, 0)) +\n             geom_abline(linewidth = .75, intercept = 0,slope = 1) + \n             ylab(\"LLF Predictions (Y), 95% Conformal Intervals\") + \n             xlab(\"Y (Real, Test Set)\") + \n             ggtitle(\"LL Forests w/ Conformal Bands\") +\n             ggthemes::theme_few()+ \n             guides(color=guide_legend(title=\"Cover Truth? (Blue = Yes)\")) + \n             scale_color_manual(values = c(\"firebrick1\",\"dodgerblue\"))\n# \nggpubr::ggarrange(ll.plot,ll.plot.c,nrow = 1,common.legend = TRUE,legend = \"bottom\") \n\n\n\n\nAs the figure makes clear, the conformal bands achieve a much better coverage rate relative to the Gaussian confidence bands. Indeed, in this particular example the conformal approach reaches exactly the desired 95% coverage, while the Gaussian confidence bands achieve only 28%.\n\n\n\n\nReferences\n\nFriedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. 2020. “Local Linear Forests.” Journal of Computational and Graphical Statistics 30 (2): 503–17.\n\n\nFriedman, Jerome H. 1991. “Multivariate Adaptive Regression Splines.” The Annals of Statistics 19 (1): 1–67.\n\n\nLei, Jing, Max G’Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. 2018. “Distribution-Free Predictive Inference for Regression.” Journal of the American Statistical Association 113 (523): 1094–1111.\n\n\nSamii, Cyrus. 2019. “Conformal Inference Tutorial.” 2019. https://cdsamii.github.io/cds-demos/conformal/conformal-tutorial.html.\n\nCitationBibTeX citation:@online{t.rametta2023,\n  author = {Jack T. Rametta},\n  title = {Conformalized {Local} {Linear} {Forests}},\n  date = {2023-08-12},\n  url = {https://cetialphafive.github.io/jrametta/posts/2023-08-12-conformal/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJack T. Rametta. 2023. “Conformalized Local Linear\nForests.” August 12, 2023. https://cetialphafive.github.io/jrametta/posts/2023-08-12-conformal/."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Conformalized Local Linear Forests\n\n\n\nconformal\n\n\nML\n\n\n\nGenerating conformal uncertainty bands for local linear regression forests\n\n\n\nJack T. Rametta\n\n\nAug 12, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Dissertation Project\nThe Lingering Effects of the Contract with America on Congressional Capacity\nMy dissertation focuses on Congressional oversight and policymaking capacity, as well as inter-branch politics. To this end, I compile two novel datasets encompassing the universe of public reports from the Congressional Research Service and the Government Accountability Office. With these datasets, I am able to derive high resolution images of the effects of the Contract with America on Congressional oversight and policymaking capacity across a wide range of substantive domains and federal agencies. These data also provide a venue to test longstanding theories of inter-branch and legislative-bureaucratic relations.\nMethods Projects\nThe Balance Permutation Test: A Machine Learning Replacement for Balance Tables (with Sam Fuller)\nUnder Review.\nMachine Learning for Subgroup Analysis: Powerful and Dangerous (with Sam Fuller)\nWorking Paper, 2023. A version of this project was presented at MPSA 2023.\nOmnibus, Cross-Validated Variable Importance Scores (with Sam Fuller)\nWorking Paper, 2022.\nSubstantive Projects\nAffect or Ideology?: The Heterogeneous Effects of Political Cues on Policy Support (with Nicolás de la Cerda and Sam Fuller)\nUnder Review. A version of this project was presented at WPSA 2023.\nHearts and Minds: Locating Voters in Affective-Ideological Space (with Christopher D. Hare and Sam Fuller)\nWorking Paper, 2023. A version of this project was presented at MPSA 2023."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Below is a list of courses I’ve assisted in teaching at the graduate and undergraduate levels. Associated instructor of record listed in the parentheses where applicable.\nGraduate Courses\nThe University of Michigan\n\nICPSR 2023: Machine Learning for Social Sciences (Summer: 2023, Christopher D. Hare)\nICPSR 2022: Machine Learning for Social Sciences (Summer: 2022, Christopher D. Hare)\n\nThe University of California, Davis\n\nPOL 213: Quantitative Analysis in Politics Science II (MLE) (S: 2022, Lauren Peritz)\nPOL 212: Quantitative Analysis in Political Science I (OLS) (W: 2022, Christopher D. Hare)\nPOL 211: Research Methods: Probability, Statistics, Design (F: 2021, Adrienne Hosek)\n\nUndergraduate Courses\n\nPOL 001: Introduction to American Politics Spring 2023 (S: 2023, Scott MacKenzie)\nPOL 051: The Scientific Study of Politics (Research Methods) (W: 2023, McCage Griffiths)\nPOL 051: The Scientific Study of Politics (Research Methods) (F: 2022, Juan Tellez)\nPOL 001: Introduction to American Politics Spring 2021 (S: 2021, Ben Highton)\nPOL 114: Quantitative Analysis of Political Data (W: 2021, Ben Highton)\nPOL 110: The Strategy of Politics (Game Theory) (F: 2020, Ryan Hübert)\nPOL 147B: British Politics (S: 2020, James F. Adams)\nPOL 012A: Politics and Sports (W: 2020, Ethan Scheiner)"
  }
]